{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCq68HyA1Yf-",
        "outputId": "c4c09b92-6de4-4977-972c-26680bf3a4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'is', 'amazing,', \"isn't\", 'it?']\n"
          ]
        }
      ],
      "source": [
        "text = \"NLP is amazing, isn't it?\"\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "I_Z3X96X4_8h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J0VN6BH5L_-",
        "outputId": "f9ed9727-a10c-4474-8aa1-78cc394549f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzDzTRcs5UKb",
        "outputId": "c54a71fd-428b-4b20-b1ce-b9657fe893b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLP is powerful. It is used everywhere!\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tDqnSYO4ykE",
        "outputId": "0c5751bc-1439-4424-ae9d-e16be82d13b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'is', 'powerful', '.', 'It', 'is', 'used', 'everywhere', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Machine learning is fun. NLP makes it even better!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ad6xi4a42JS",
        "outputId": "525fe7c9-dc8f-4032-d51a-3299a1e9bc50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine', 'learning', 'is', 'fun', '.', 'NLP', 'makes', 'it', 'even', 'better', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Tokenization splits text into words.\"\n",
        "print(word_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "untBc4Ly5akg",
        "outputId": "0f820ff4-7c39-4dd1-800e-ba6be9debba2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'splits', 'text', 'into', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"ChatGPT is transforming NLP.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho-p6rrm5dSj",
        "outputId": "ee016f67-70db-4a9f-8362-41b824e81456"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ChatGPT', 'is', 'transforming', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Deep learning improves natural language processing.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W124Gmh5gs4",
        "outputId": "66111ca2-1dcb-4c13-dc1a-74e7c6b2f7a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep\n",
            "learning\n",
            "improves\n",
            "natural\n",
            "language\n",
            "processing\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texts = [\"NLP is amazing.\", \"Word tokenization is essential.\"]\n",
        "for t in texts:\n",
        "    print(word_tokenize(t))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT9L8XFw5m6s",
        "outputId": "4bf18308-579c-460b-938a-247fadb10091"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'is', 'amazing', '.']\n",
            "['Word', 'tokenization', 'is', 'essential', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLP, AI, and ML are related fields.\"\n",
        "tokens = [w for w in word_tokenize(text) if w.isalpha()]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMGY5_T75pUS",
        "outputId": "566412fe-3d8c-4d3a-9cc8-6961f6082421"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'AI', 'and', 'ML', 'are', 'related', 'fields']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Cats are chasing the mice.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "tokens = [token.lemma_ for token in doc]\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBKy4doO5sM0",
        "outputId": "3687d8ae-c1f9-4ff0-ede1-45567bc90d4b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'be', 'chase', 'the', 'mouse', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/84/84-0.txt\"\n",
        "text = requests.get(url).text\n",
        "\n",
        "tokens = text.split()\n",
        "print(tokens[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nJZqIwi5vlN",
        "outputId": "a8524851-f3cb-4422-c4ac-edd847e73951"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['***', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', '84', '***', 'Frankenstein;', 'or,', 'the', 'Modern', 'Prometheus', 'by', 'Mary', 'Wollstonecraft', '(Godwin)', 'Shelley', 'CONTENTS', 'Letter', '1', 'Letter', '2', 'Letter', '3', 'Letter', '4', 'Chapter', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = requests.get(\"https://www.gutenberg.org/files/2701/2701-0.txt\").text\n",
        "print(word_tokenize(text[:300]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCRGo0mR53hU",
        "outputId": "287eaeef-3153-4229-daba-67307adb7b76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['*', '*', '*', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', '2701', '*', '*', '*', 'MOBY-DICK', ';', 'or', ',', 'THE', 'WHALE', '.', 'By', 'Herman', 'Melville', 'CONTENTS', 'ETYMOLOGY', '.', 'EXTRACTS', '(', 'Supplied', 'by', 'a', 'Sub-Sub-Librarian', ')', '.', 'CHAPTER', '1', '.', 'Loomings', '.', 'CHAPTER', '2', '.', 'The', 'Carpet-Bag', '.', 'CHAPTER', '3', '.', 'The', 'Spouter-Inn', '.', 'CHAPTER', '4', '.', 'The', 'Counterpan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = requests.get(\"https://www.gutenberg.org/cache/epub/11/pg11.txt\").text\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:40])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH0O93O-57pe",
        "outputId": "d7f012af-5a26-47ef-88bb-c8703f83508e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = requests.get(\"https://www.gutenberg.org/files/98/98-0.txt\").text\n",
        "doc = nlp(text[:500])\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuNWtmvm5-vN",
        "outputId": "05fd87f1-9138-474e-d8a4-63478976d2ff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï»¿The\n",
            "Project\n",
            "Gutenberg\n",
            "eBook\n",
            "of\n",
            "A\n",
            "Tale\n",
            "of\n",
            "Two\n",
            "Cities\n",
            ",\n",
            "by\n",
            "Charles\n",
            "Dickens\n",
            "\r\n",
            "\r\n",
            "\n",
            "This\n",
            "eBook\n",
            "is\n",
            "for\n",
            "the\n",
            "use\n",
            "of\n",
            "anyone\n",
            "anywhere\n",
            "in\n",
            "the\n",
            "United\n",
            "States\n",
            "and\n",
            "\r\n",
            "\n",
            "most\n",
            "other\n",
            "parts\n",
            "of\n",
            "the\n",
            "world\n",
            "at\n",
            "no\n",
            "cost\n",
            "and\n",
            "with\n",
            "almost\n",
            "no\n",
            "restrictions\n",
            "\r\n",
            "\n",
            "whatsoever\n",
            ".\n",
            "You\n",
            "may\n",
            "copy\n",
            "it\n",
            ",\n",
            "give\n",
            "it\n",
            "away\n",
            "or\n",
            "re\n",
            "-\n",
            "use\n",
            "it\n",
            "under\n",
            "the\n",
            "terms\n",
            "\r\n",
            "\n",
            "of\n",
            "the\n",
            "Project\n",
            "Gutenberg\n",
            "License\n",
            "included\n",
            "with\n",
            "this\n",
            "eBook\n",
            "or\n",
            "online\n",
            "at\n",
            "\r\n",
            "\n",
            "www.gutenberg.org\n",
            ".\n",
            "If\n",
            "you\n",
            "are\n",
            "not\n",
            "located\n",
            "in\n",
            "the\n",
            "United\n",
            "States\n",
            ",\n",
            "you\n",
            "\r\n",
            "\n",
            "will\n",
            "have\n",
            "to\n",
            "check\n",
            "the\n",
            "laws\n",
            "of\n",
            "the\n",
            "country\n",
            "where\n",
            "you\n",
            "are\n",
            "located\n",
            "b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = requests.get(\"https://www.gutenberg.org/files/1342/1342-0.txt\").text\n",
        "lines = text.split(\"\\n\")\n",
        "\n",
        "for line in lines[:5]:\n",
        "    print(word_tokenize(line))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHHZDIM96A2P",
        "outputId": "1f6ba092-dcc5-40dd-d4fb-84a1061ad8ab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['*', '*', '*', 'START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', '1342', '*', '*', '*']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = requests.get(\"https://www.gutenberg.org/files/64317/64317-0.txt\").text\n",
        "text = text.lower()\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:25])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDEMr2jc6FAq",
        "outputId": "355ec5ee-2b7b-4c10-9f60-19eb4c1cdfc7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['*', '*', '*', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', '64317', '*', '*', '*', 'the', 'great', 'gatsby', 'by', 'f.', 'scott', 'fitzgerald', 'table', 'of', 'contents', 'i', 'ii']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = requests.get(\"https://www.gutenberg.org/files/1400/1400-0.txt\").text\n",
        "tokens = [t for t in word_tokenize(text) if t.isalpha()]\n",
        "print(tokens[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qSsPF6T6Hd4",
        "outputId": "538839ae-710a-4253-e932-f1fe02cb2b39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['START', 'OF', 'THE', 'PROJECT', 'GUTENBERG', 'EBOOK', 'Illustration', 'Great', 'Expectations', 'Edition', 'by', 'Charles', 'Dickens', 'Contents', 'Chapter', 'I', 'Chapter', 'II', 'Chapter', 'III', 'Chapter', 'IV', 'Chapter', 'Chapter', 'VI', 'Chapter', 'VII', 'Chapter', 'VIII', 'Chapter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = requests.get(\"https://www.gutenberg.org/files/16328/16328-0.txt\").text\n",
        "doc = nlp(text[:500])\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNUsWbF86Kaq",
        "outputId": "7cf86774-d9d8-4386-836c-7c49426e1cb9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['*', '*', '*', 'start', 'of', 'the', 'project', 'GUTENBERG', 'EBOOK', '16328', '*', '*', '*', '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n', 'BEOWULF', '\\r\\n', 'AN', 'ANGLO', '-', 'SAXON', 'EPIC', 'POEM', '\\r\\n\\r\\n', '_', 'translate', 'from', 'the', 'HEYNE', '-', 'SOCIN', 'text', '_', '\\r\\n\\r\\n', 'BY', '\\r\\n\\r\\n', 'JNO', ':', 'LESSLIE', 'HALL', ',', 'Ph', '.', 'D.', '(', 'J.H.U.', ')', '\\r\\n\\r\\n', 'Professor', 'of', 'English', 'and', 'history', 'in', 'the', 'College', 'of', 'William', 'and', 'Mary', '\\r\\n\\r\\n\\r\\n', 'D.C.', 'HEATH', '&', 'CO', '.', ',', 'PUBLISHERS', '\\r\\n', 'BOSTON', 'NEW', 'YORK', 'CHICAGO', '\\r\\n\\r\\n\\r\\n', 'enter', 'accord', 'to', 'Act', 'of', 'Congress', ',', 'in', 'the', 'year', '1892', ',', 'by', '\\r\\n\\r\\n', 'JNO', ':', 'LESSLIE', 'HALL', ',', '\\r\\n\\r\\n', 'in', 'the', 'Office', 'of', 'the', 'Librarian', 'of', 'Congress', ',', 'at', 'Washington', '.', '\\r\\n\\r\\n\\r\\n', 'to', '\\r\\n\\r\\n', 'my', 'wife', '\\r\\n\\r\\n', '[', 'v', ']', '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n', 'CONTENT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = requests.get(\"https://www.gutenberg.org/files/1998/1998-0.txt\").text\n",
        "doc = nlp(text[:800])\n",
        "\n",
        "tokens = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UThVzlSL6Nw0",
        "outputId": "43b9ef26-f1d9-434e-c811-150e9e0a2fe2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Project', 'use', 'parts', 'world', 'cost', 'restrictions', 'terms', 'laws', 'country', 'Title', 'None', 'Author', 'Language', 'encoding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANp7EOFH6QLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}